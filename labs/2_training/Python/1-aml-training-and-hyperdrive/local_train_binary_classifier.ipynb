{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn binary classification model training on local notebook. \n",
    "The code is plain vanilla Scikit-Learn training/creation of a binary classification model.Azure ML is only used to gather original data from an AML Dataset. This notebook can run on a local PC or on any Azure ML Compute Instance or Azure ML Compute Instance.\n",
    "\n",
    "*This code is the baseline for the next labs/notebooks in the Workshop moving to AML remote compute, etc*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries to use in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure SDK version: 1.19.0\n",
      "The scikit-learn version is 0.22.2.post1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Dataset, Environment, Experiment\n",
    "import joblib\n",
    "\n",
    "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Workspace to load Tabular Datasets and to log local training into AML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to an existing Azure ML Workshop in order to use Azure ML Datasets and Runs Logging into AML\n",
    "# if you run this locally download config.json and place it in root folder of the repo\n",
    "# ws = Workspace.from_config('../../../config.json') \n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('../../../../../../Data/datasets/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the German credit data\n",
    "\n",
    "Note: as you are now accessing the workspace, the Notebook needs to be authenticated for access through device authentication. Hence, you will be prompted with a device login like so: \n",
    "\n",
    "    Performing interactive authentication. Please follow the instructions on the terminal.\n",
    "    To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code XXXXX to authenticate.\n",
    "    \n",
    "Please follow these instructions in a new browser tab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Azure ML, *datastores* are references to storage locations, such as Azure Storage blob containers. Every workspace has a default datastore - usually the Azure storage blob container that was created with the workspace. If you need to work with data that is stored in different locations, you can add custom datastores to your workspace and set any of them to be the default.\n",
    "\n",
    "Run the following code to determine the datastores in your workspace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "# Get the default datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Enumerate all datastores, indicating which is the default\n",
    "for ds_name in ws.datastores:\n",
    "    print(ds_name, \"- Default =\", ds_name == default_ds.name)\n",
    "\n",
    "default_ds.upload(src_dir='../../../../../../Data/datasets/',\n",
    "                 target_path='datasets',\n",
    "                 overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and Register a Dataset\n",
    "A dataset is an object that encapsulates a specific data source. Let's create a dataset from the german credit data you uploaded to the datastore, and view the first 3 records. In this case, the data is in a structured format in a CSV file, so we'll use a Tabular dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Dataset\n",
    "\n",
    "# Create a tabular dataset from the path on the datastore (this may take a short while)\n",
    "dataset = Dataset.Tabular.from_delimited_files(path = [(default_ds, './datasets/german_credit_data.csv')])\n",
    "\n",
    "# preview the first 3 rows of the dataset\n",
    "dataset.take(3).to_pandas_dataframe()\n",
    "\n",
    "creditData = dataset.register(workspace = ws,\n",
    "                                name = 'german-credit',\n",
    "                                description = 'german credit data',\n",
    "                                create_new_version = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the German credit dataset from the workspace\n",
    "creditData = ws.datasets['german-credit'].to_pandas_dataframe()\n",
    "creditData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping Sno column since it is merely an identifier\n",
    "target = creditData[\"Risk\"]\n",
    "creditData = creditData.drop(['Sno'], axis=1)\n",
    "creditXData = creditData.drop(['Risk'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split in Train and Test datasets (DataFrames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(creditXData, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the categorical and numerical column names in separate lists\n",
    "categorical = []\n",
    "for col, value in creditXData.iteritems():\n",
    "    if value.dtype == 'object':\n",
    "        categorical.append(col)\n",
    "        \n",
    "numerical = creditXData.columns.difference(categorical)\n",
    "\n",
    "print(numerical)\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data processing pipelines (Scikit-Learn pipelines)\n",
    "NOTE: This code uses Scikit-Learn pipelines. Not related to AML Pipelines. Different concept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# We create the transformations pipelines for both numeric and categorical data.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
    "\n",
    "transforms_pipeline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numerical),\n",
    "        ('cat', categorical_transformer, categorical)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add classifier algorithm (SVC: Support Vector Classifier) to the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append classifier to Scikit-Learn transformations pipeline.\n",
    "# Now we have a full Scikit-Learn prediction pipeline.\n",
    "model_pipeline = Pipeline(steps=[('preprocessor', transforms_pipeline),\n",
    "                      ('classifier', SVC(kernel='linear', C = 1.0, probability=True))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Experiment, run and log just for logging info while training locally in Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Get an experiment object from AML\n",
    "experiment = Experiment(workspace=ws, name=\"local-train-german-credit\")\n",
    "\n",
    "# Create a run object in the experiment\n",
    "run =  experiment.start_logging()\n",
    "# Log the algorithm parameter C to the run\n",
    "run.log('C', 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the SVM (Support Vector Machine) Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model_pipeline.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions and calculate Accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Make Multiple Predictions\n",
    "y_predictions = model.predict(x_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predictions)\n",
    "print('Accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show confusion matrix in a separate window\n",
    "plt.matshow(cm)\n",
    "plt.title('Confusion matrix')\n",
    "plt.colorbar()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log metric and model into the AML run definition\n",
    "### Note that training is local, we just use the run definition to log information about the run/training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Image\n",
    "run.log_image('Plot', plot=plt)\n",
    "\n",
    "# Output the Accuracy to the notebook and to the run\n",
    "run.log('accuracy', accuracy)\n",
    "\n",
    "# Save the model to the outputs directory for capture\n",
    "model_file_name = 'outputs/model.pkl'\n",
    "joblib.dump(value = model, filename = model_file_name)\n",
    "\n",
    "# Complete the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the experiment run and its logged info in Azure ML Workspace\n",
    "Now, you should go to your AML Workspace and check the information logged for this run, such as the accuracy, hyper-parameters and any other info you logged for the experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Prediction\n",
    "instance_num = 1\n",
    "\n",
    "# Get the prediction for the first member of the test set and explain why model made that prediction\n",
    "prediction_value = model.predict(x_test)[instance_num]\n",
    "print(\"One Prediction: \")\n",
    "print(prediction_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.concat([x_test, y_test], axis=1)\n",
    "predictions['prediction'] = y_predictions\n",
    "predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.model import Model\n",
    "model = Model.register(workspace = ws,    \n",
    "                       model_path = \"./outputs/model.pkl\", # this points to a local file\n",
    "                       model_name = \"creditmodel_local\", # this is the name the model is registered as\n",
    "                       tags = {'area': \"German credit risk\", 'type': \"Financial Services\"},\n",
    "                       description = \"Binary classification model to understand credit risk based on the German credit risk data\",\n",
    "                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
