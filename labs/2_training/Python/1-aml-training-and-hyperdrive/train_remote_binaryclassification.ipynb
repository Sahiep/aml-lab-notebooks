{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-Learn binary classification model.  Remote train via Azure ML Compute (AML Cluster) \n",
        "## and optional HyperDrive (hyper-parameter tuning with multiple child runs) \n",
        "\n",
        "This notebook showcases the creation of a ScikitLearn Binary classification model by remotely training on Azure ML Compute Target (AMLCompute Cluster). It shows multiple ways of remote training like using a single Estimator, a ScriptRunConfig and hyper-parameter tunning with HyperDrive with multiple child trainings"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check library versions\n",
        "This is important when interacting with different executions between remote compute environments (cluster) and the instance/VM with the Jupyter Notebook.\n",
        "If not using the same versions you can have issues when creating .pkl files in the cluster and downloading them to load it in the Jupyter notebook."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Check versions\n",
        "import azureml.core\n",
        "import sklearn\n",
        "import joblib\n",
        "import pandas \n",
        "\n",
        "print(\"Azure SDK version:\", azureml.core.VERSION)\n",
        "print('scikit-learn version: {}.'.format(sklearn.__version__))\n",
        "print('joblib version: {}.'.format(joblib.__version__))\n",
        "print('pandas version: {}.'.format(pandas.__version__))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618816988999
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and connect to AML Workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Workspace\n",
        "\n",
        "#ws = Workspace.from_config('../../../config.json') # if you run this locally download config.json and place it in root folder of the repo\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "print(ws.name, ws.resource_group, ws.location, sep='\\n')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817760763
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create An Experiment\n",
        "\n",
        "**Experiment** is a logical container in an Azure ML Workspace. It hosts run records which can include run metrics and output artifacts from your experiments."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Experiment\n",
        "\n",
        "experiment_name = 'amlcompute-train-german-credit'\n",
        "experiment = Experiment(workspace=ws, name=experiment_name)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817019099
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Azure ML Compute\n",
        "\n",
        "Azure ML supports a range of compute targets, which you can define in your workpace and use to run experiments; paying for the resources only when using them."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create project directory and copy the training script into the project directory\n",
        "\n",
        "Create a directory that will contain all the necessary code from your local machine that you will need access to on the remote resource. This includes the training script, and any additional files your training script depends on"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "project_folder = './amlcompute-train-german-credit'\n",
        "os.makedirs(project_folder, exist_ok=True)\n",
        "\n",
        "# Copy the training script into the project directory\n",
        "shutil.copy('train.py', project_folder)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817019325
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Connect or Create a Remote AML compute cluster\n",
        "\n",
        "Try to use the compute target you had created before (make sure you provide the same name here in the variable `cpu_cluster_name`).\n",
        "If not available, create a new cluster from the code."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "# Choose a name for your CPU cluster\n",
        "cpu_cluster_name = \"cpu-cluster\"\n",
        "\n",
        "# Verify that cluster does not exist already\n",
        "try:\n",
        "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
        "    print('Found existing cluster, use it.')\n",
        "except ComputeTargetException:\n",
        "    # Create an AzureMl Compute resource (a container cluster)\n",
        "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_DS2_V2', \n",
        "                                                           vm_priority='dedicated', \n",
        "                                                           max_nodes=2)\n",
        "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
        "\n",
        "cpu_cluster.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817040391
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fetch the AML Dataset"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "aml_dataset = ws.datasets['german-credit']"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817040634
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Environment \n",
        "#### Optionally list all the available environments and packages in your AML Workspace"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core import Environment\n",
        "\n",
        "envs = Environment.list(workspace=ws)\n",
        "\n",
        "# List Environments and packages in my workspace\n",
        "#for env in envs:\n",
        "    #if env.startswith(\"AzureML\"):\n",
        "        #print(\"Name\", env)\n",
        "        #print(\"packages\", envs[env].python.conda_dependencies.serialize_to_string())\n",
        "        \n",
        "# Use curated environment from AML named \"AzureML-Tutorial\"\n",
        "curated_environment = Environment.get(workspace=ws, name=\"AzureML-Tutorial\")\n",
        "#print(\"packages\", curated_environment.python.conda_dependencies.serialize_to_string())\n",
        "\n",
        "# Custom environment: \n",
        "# Environment.get(workspace=ws, name=\"myenv\", version=\"1\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817040975
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure & Run using ScriptRunConfig & Environment \n",
        "(Easiest path using curated environments)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Add training script to run config\n",
        "from azureml.core import ScriptRunConfig, RunConfiguration, Experiment\n",
        "\n",
        "script_runconfig = ScriptRunConfig(source_directory=project_folder, \n",
        "                                   script=\"train.py\",\n",
        "                                   arguments=[aml_dataset.as_named_input('credit')]\n",
        "                                  )\n",
        "\n",
        "# Attach compute target to run config\n",
        "script_runconfig.run_config.target = cpu_cluster\n",
        "# runconfig.run_config.target = \"local\"\n",
        "\n",
        "# Attach environment to run config\n",
        "script_runconfig.run_config.environment = curated_environment"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817041129
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run the experiment with a single ScriptRunConfig and single run (Optional Jump)\n",
        "Jump this step if you want to use HyperDrive (Go to Option C)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Submit the Experiment Run to the AML Compute \n",
        "run = experiment.submit(script_runconfig)\n",
        "run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817047241
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Monitor Run "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817047481
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Get log results upon completion\n",
        "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use wait_for_completion to show when the model training is finished:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "run.wait_for_completion(show_output=True)  # specify True for a verbose log"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": [],
        "gather": {
          "logged": 1618817393486
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure and Run with Intelligent hyperparameter tuning (HyperDrive using Estimator)\n",
        "\n",
        "The adjustable parameters that govern the training process are referred to as the hyperparameters of the model. The goal of hyperparameter tuning is to search across various hyperparameter configurations and find the configuration that results in the best performance.\n",
        "\n",
        "To demonstrate how Azure Machine Learning can help you automate the process of hyperarameter tuning, we will launch multiple runs with different values for numbers in the sequence. First let's define the parameter space using random sampling."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create a hyperparameter sweep\n",
        "First, we will define the hyperparameter space to sweep over. \n",
        "In this example we will use random sampling to try different configuration sets of hyperparameters to maximize our primary metric, Accuracy."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# from azureml.train.hyperdrive import *\n",
        "from azureml.train.hyperdrive import RandomParameterSampling, BayesianParameterSampling \n",
        "from azureml.train.hyperdrive import BanditPolicy, HyperDriveConfig, PrimaryMetricGoal\n",
        "from azureml.train.hyperdrive import choice, loguniform\n",
        "    \n",
        "# solver{'newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'}, default='lbfgs'\n",
        "# penalty{'l1', 'l2', 'elasticnet', 'none'}, default='l2' --- Note that some penalty parameters are not supported by some algorithms..\n",
        "param_sampling = RandomParameterSampling({\n",
        "                                          \"--solver\": choice('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
        "                                          \"--penalty\": choice('l2')\n",
        "                                         }\n",
        "                                        )\n",
        "# Details on Scikit-Learn LogisticRegression hyper-parameters:\n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817393993
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will define an early termination policy. The BanditPolicy basically states to check the job every 2 iterations. If the primary metric (defined later) falls outside of the top 10% range, Azure ML terminate the job. This saves us from continuing to explore hyperparameters that don't show promise of helping reach our target metric.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n",
        "# Note that early termination policy is currently NOT supported with Bayesian sampling\n",
        "# Check here for recommendations on the multiple policies:\n",
        "# https://docs.microsoft.com/en-us/azure/machine-learning/how-to-tune-hyperparameters#picking-an-early-termination-policy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817394626
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to configure a run configuration object, and specify the primary metric 'Accuracy' that's recorded in your training runs. \n",
        "If you go back to visit the training script, you will notice that this value is being logged. \n",
        "We also want to tell the service that we are looking to maximizing this value. \n",
        "We also set the number of samples to 20, and maximal concurrent job to 4."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Note that in this case when using HyperDrive, the original Estimator's parameters are not used but the HyperDrive parameters...\n",
        "hyperdrive_config = HyperDriveConfig(run_config=script_runconfig, \n",
        "                                     hyperparameter_sampling=param_sampling, \n",
        "                                     policy=early_termination_policy,\n",
        "                                     primary_metric_name='Accuracy',\n",
        "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
        "                                     max_total_runs=10,\n",
        "                                     max_concurrent_runs=4)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817395005
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Lauch the hyperparameter tuning job."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# start the HyperDrive run\n",
        "hyperdrive_run = experiment.submit(hyperdrive_config)\n",
        "\n",
        "# Check here how to submit the hyperdrive run as a step of an AML Pipeline:\n",
        "# https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/machine-learning-pipelines/intro-to-pipelines/aml-pipelines-parameter-tuning-with-hyperdrive.ipynb"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817395411
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Monitor the HyperDrive run\n",
        "\n",
        "Monitor the progress of the runs with the Jupyter widget."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.widgets import RunDetails\n",
        "RunDetails(hyperdrive_run).show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817396660
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get details for debugging:\n",
        "RunDetails(hyperdrive_run).get_widget_data()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817398150
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Get log results upon completion\n",
        "Model training and monitoring happen in the background. Wait until the model has finished training before you run more code. Use wait_for_completion to show when the model training is finished:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "hyperdrive_run.wait_for_completion(show_output=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817753356
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Find and get the best model found by HyperDriveÂ¶ \n",
        "When all jobs finish, we can find out the one that has the highest accuracy."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
        "print(best_run.get_details()['runDefinition']['arguments'])\n",
        "# print(best_run.get_details())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817753943
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy 'best_run' to 'run' to re-use the same code also used without HyperDrive\n",
        "run = best_run"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817754484
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Display run metrics results\n",
        "You now have a model trained on a remote cluster. Retrieve the accuracy of the model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_metrics())"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817755040
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## See files associated with the run"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(run.get_file_names())\n",
        "\n",
        "run.download_file('azureml-logs/70_driver_log.txt')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817756328
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Scikit-Learn model"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve model for visualization and deployment\n",
        "# Download the model .pkl file to local (Using the 'run' object)\n",
        "run.download_file('outputs/model.pkl')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817757084
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Register the model\n",
        "Once you've trained the model, you can save and register it to your workspace. Model registration lets you store and version your models in your workspace to simplify model management and deployment.\n",
        "\n",
        "Running the following code will register the model to your workspace, and will make it available to reference by name in remote compute contexts or deployment scripts. "
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from azureml.core.model import Model\n",
        "\n",
        "model_reg = run.register_model(model_name='creditmodel_aml',    # Name of the registered model in your workspace.\n",
        "                               description='Binary classification model for German credit risk data',\n",
        "                               model_path='outputs/model.pkl', # Local file to upload and register as a model.\n",
        "                               model_framework=Model.Framework.SCIKITLEARN,     # Framework used to create the model.\n",
        "                               model_framework_version='0.20.3',                # Version of scikit-learn used to create the model.\n",
        "                               tags={'ml-task': \"binary-classification\", 'business-area': \"Risk\"},\n",
        "                               properties={'joblib-version': \"0.14.0\", 'pandas-version': \"0.23.4\"},\n",
        "                               sample_input_dataset=aml_dataset\n",
        "                              )\n",
        "model_reg"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817757620
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Scikit-Learn model pickle file from the model registry\n",
        "from azureml.core.model import Model\n",
        "print(Model.get_model_path('creditmodel_aml', _workspace=ws))\n",
        "\n",
        "model_from_registry = Model(ws,'creditmodel_aml')\n",
        "model_from_registry.download(target_dir='.', exist_ok=True)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1618817760077
        }
      }
    }
  ],
  "metadata": {
    "pygments_lexer": "ipython3",
    "name": "python",
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "mimetype": "text/x-python",
    "npconvert_exporter": "python",
    "kernel_info": {
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "file_extension": ".py",
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}